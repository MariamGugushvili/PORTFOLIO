# ğŸ§  Deep Reinforcement Learning â€“ Autonomous Driving with PPO

## ğŸ“„ Project Overview

This project applies **Deep Reinforcement Learning (DRL)** to teach an agent how to drive a car around a racetrack autonomously using Proximal Policy Optimization (PPO). The agent learns from raw visual input (grayscale frames) and receives rewards based on driving efficiency.

---

## ğŸ—ï¸ Technical Architecture

- **Environment:** `CarRacing-v3` from OpenAI Gymnasium  
- **State:** Stacked grayscale frames (4x84x84)  
- **Action Space:** Continuous â€“ steering, acceleration, braking  
- **Reward Shaping:** Penalized erratic driving and off-track behavior  
- **Model:** CNN-based Actor-Critic (PPO)  
- **Training Tool:** Stable Baselines3 with EvalCallback & TensorBoard  
- **Evaluation:** Average reward per episode, entropy, policy loss, value loss  

---

## ğŸ§° Tools & Stack

- Python  
- Stable Baselines3 (PPO)  
- Gymnasium (`CarRacing-v3`)  
- TensorBoard  
- OpenCV  
- Matplotlib

---

## ğŸ–¼ï¸ Screenshots or Visuals

### ğŸ•¹ï¸ Agent View â€“ Frame Sample  
![Agent Frame](./images/Frames%20from%20Simulation.png)  
> Processed grayscale frame captured during PPO training from the CarRacing-v3 environment.

---

### ğŸ“ˆ Average Episode Reward Over Time  
![Reward Curve](./images/Mean_Reward_PPO.png)  
> Reward increased steadily across training. Final agent averaged over **750** reward per episode.

---

### ğŸ“‰ PPO Loss Curves â€“ Value, Policy, Entropy  
![Loss Curves](./images/Value_Loss_Policy_Loss.png)  
> Tracks the evolution of value loss, policy loss, and entropy â€” used to tune learning behavior.

---

### ğŸ§ª Model Comparison (Optional)  
![Model Comparison](./images/Comparison.png)  
> Summary of PPO modelâ€™s performance compared to other approaches (if relevant).

---

## ğŸ” Additional Notes

- Reward shaping was key to training stability  
- Domain randomization improved robustness across different track layouts  
- Ablation testing showed smaller batch sizes led to better exploration early on

---

## ğŸš€ Outcomes

- âœ… Trained an agent using PPO to drive autonomously in the `CarRacing-v3` environment  
- âœ… Achieved **average reward ~750+** after tuning entropy, learning rate, and episode steps  
- âœ… Successfully shaped reward to discourage zig-zag movement and off-road driving  
- âœ… Implemented custom CNN-based policy/value networks  
- âœ… Used evaluation callbacks and TensorBoard for monitoring model performance  

