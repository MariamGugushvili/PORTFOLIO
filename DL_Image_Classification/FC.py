# -*- coding: utf-8 -*-
"""+++Task1a.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R0Eu-i-3ETrSyyladKSuff5tbbMPRVVO

# **FULLY CONNECTED MODEL**

These lines import classes for loading the CIFAR-10 dataset, building a sequential model, and using the Input, Dense, and Flatten layers.
"""

from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Flatten
from tensorflow.keras.utils import plot_model
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report

"""Import the CIFAR-10 and print shapes of test and train datasets"""

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

"""I normalized the pixel values to be between 0 and 1, which can improve model performance."""

# Normalize the pixel values
X_train_normalized = X_train / 255.0
X_test_normalized = X_test / 255.0

"""Create a Model:
1. Flatten Layer: This layer flattens the 2D image into a 1D array.
2. Dense Layers: These are fully connected layers. The first two layers with ReLU activation extract features from the input.
3. Output Layer: The final layer has 10 neurons with softmax activation, outputting probabilities for each class.


"""

model = Sequential([
    Input(shape=(32, 32, 3)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])

plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)

"""**optimizer='adam':** Adam optimizer for efficient gradient descent.

**loss='sparse_categorical_crossentropy':** Loss function suitable for multi-class classification.

**metrics=['accuracy']:** We monitor accuracy during training
"""

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train_normalized, y_train, epochs=20, validation_data=(X_test_normalized, y_test))

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Function Evaluation')
plt.legend()

plt.show()

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy Function Evaluation')
plt.legend()

plt.show()

y_pred_probs = model.predict(X_test_normalized)
y_pred = np.argmax(y_pred_probs, axis=1)
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='YlGn')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, y_pred))